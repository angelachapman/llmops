{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VEzqziR6yt",
        "outputId": "f873dd3b-55a0-4e00-ecf4-e2a0fe3af327"
      },
      "outputs": [],
      "source": [
        "!pip install -qU numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Pmarca Blog Archives\n",
            "(select posts from 2007-2009)\n",
            "Marc Andreessen\n",
            "copyright: Andreessen Horow\n"
          ]
        }
      ],
      "source": [
        "print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=2000)\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivity 127\\nPSYCHOLOGY AND ENTREPRENEURSHIP\\nThe Psychology of Entrepreneurial Misjudgment:\\nBiases 1-6\\n142\\nAge and the Entrepreneur: Some data 154\\nLuck and the entrepreneur: The four kinds of luck 162\\nSerial Entrepreneurs 168\\nTHE BACK PAGES\\nTop 10 science Dction novelists of the \\'00s ... so far\\n(June 2007)\\n173\\nBubbles on the brain (October 2009) 180\\nOK, you\\'re right, it IS a bubble (October 2009) 186\\nThe Pmarca Guide to\\nStartups\\nPart 1: Why not to do a startup\\nIn this series of posts I will walk through some of my accumulated knowledge and experience in building high-tech startups.\\nMy speciXc experience is from three companies I have cofounded: Netscape, sold to America Online in 1998 for $4.2\\nbillion; Opsware (formerly Loudcloud), a public soaware company with an approximately $1 billion market cap; and now\\nNing, a new, private consumer Internet company.\\nBut more generally, I’ve been fortunate enough to be involved\\nin and exposed to a broad range of other startups — maybe 40\\nor 50 in enough']"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Yes, it can be reduced by setting the \"dimensions\" API parameter\n",
        "2. Matryoshka Representation Learning (MRL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using async for embeddings to prevent blocking the application during the potentially slow I/O operation of fetching embeddings, thereby improving responsiveness and efficiency of the application. Using async calls is a common design pattern that helps improve the responsiveness of LLM-heavy applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ot the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              "  np.float64(0.5140192709554194)),\n",
              " ('management,\\nmarketing, or sales. You don’t want your key executives focused\\non selling the company — unless of course you want them\\nfocused on selling the company. Make your acceleration decisions accordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — consciously or subconsciously. Don’t let it happen to you — make sure the person you\\nhire into that role is way better than you used to be.\\nEighth, recognize that hiring an executive is a high-risk proposition.\\nYou oaen see a startup with a screwed up development process,\\nbut “when we get our VP of Engineering onboard, everything\\nwill get Xxed”. Or a startup that is missing its revenue targets, but\\n“when we get our VP of sales, reveue will take oW”.\\nHere’s the problem: in my experience, if you know what you’re\\ndoing, the odds of a given executive hire working out will be about\\nPart 8: Hiring, managing, promoting, and Dring executives 59\\n50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\\nyou’re doing, your failure rate will b',\n",
              "  np.float64(0.49131577421255573)),\n",
              " (' describes a concept\\ncalled “Task Relevant Maturity”. Andy explains that employees\\nwho are immature in a given task require detailed training and\\ninstruction. They need to be micromanaged. On the other\\nhand, if an employee is relatively mature in a task, then it is\\ncounterproductive and annoying to manage the details of their\\nwork.\\nThis is also true when managing executives. Marc might think\\nthat he hires an executive because she has the experience and\\nknow-how to comprehensively do her job, so any detailed\\ninstruction would be unwise and unwarranted. Marc would be\\nwrong about that. It turns out that even — and maybe especially\\n— executives are also immature in certain tasks.\\nIt is almost always the case that a new executive will be immature in their understanding of your market, your technology,\\nand your company — its personnel, processes, and culture. Will\\nthe new head of engineering at Ning walk in the door with\\nMarc’s understanding of the development process or the technology base? Would it be better for this new head of engineering\\nto make guesses and use her own best — not so good– judgment, or for Marc to review the Xrst say 20 decisions until the\\nnew exec is fully up to speed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthe',\n",
              "  np.float64(0.45662211002355796))]"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI recently introduced the *seed* input parameter, which can be used to produce \"mostly\" reproducible responses to a given prompt as long as temperature and top p are also kept the same. They also provide access to the *system_fingerprint* repsonse parameter which indicates model versioning on their side and helps make it a little more transparent when responses change due to system updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop in Python often depends on the specific task you want to accomplish, but there are some general guidelines to ensure your loops are clear, efficient, and easy to understand. Here are a few tips, along with examples for common loop types:\n",
            "\n",
            "### 1. **Using `for` loops for finite iterations**\n",
            "\n",
            "If you know the number of iterations in advance, the `for` loop is usually the most straightforward choice. It’s concise and readable.\n",
            "\n",
            "```python\n",
            "# Example: Print numbers 0 to 9\n",
            "for i in range(10):\n",
            "    print(i)\n",
            "```\n",
            "\n",
            "### 2. **Using `while` loops for indefinite iterations**\n",
            "\n",
            "When the number of iterations is not known in advance, a `while` loop can be useful. Make sure to include a condition that eventually stops the loop to avoid infinite loops.\n",
            "\n",
            "```python\n",
            "# Example: Print numbers until a certain condition is met\n",
            "count = 0\n",
            "while count < 10:\n",
            "    print(count)\n",
            "    count += 1  # Don't forget to increment to avoid an infinite loop!\n",
            "```\n",
            "\n",
            "### 3. **List comprehensions for concise iterations**\n",
            "\n",
            "For cases where you're generating lists or want to transform data, list comprehensions can be a more Pythonic way to write your loops.\n",
            "\n",
            "```python\n",
            "# Example: Create a list of squares\n",
            "squares = [x ** 2 for x in range(10)]\n",
            "print(squares)\n",
            "```\n",
            "\n",
            "### 4. **Using `enumerate` for index-value pairs**\n",
            "\n",
            "When you need both the index and the value from a sequence, use `enumerate()`, which makes your code cleaner and avoids manually managing the index.\n",
            "\n",
            "```python\n",
            "# Example: Print index-value pairs in a list\n",
            "fruits = ['apple', 'banana', 'cherry']\n",
            "for index, fruit in enumerate(fruits):\n",
            "    print(f\"Index {index}: {fruit}\")\n",
            "```\n",
            "\n",
            "### 5. **Using `zip` for parallel iteration**\n",
            "\n",
            "If you have multiple lists that you want to iterate over in parallel, `zip()` can be very handy.\n",
            "\n",
            "```python\n",
            "# Example: Pair two lists together\n",
            "names = ['Alice', 'Bob', 'Charlie']\n",
            "scores = [85, 90, 95]\n",
            "\n",
            "for name, score in zip(names, scores):\n",
            "    print(f\"{name} scored {score}\")\n",
            "```\n",
            "\n",
            "### 6. **Avoiding deep nesting**\n",
            "\n",
            "Deep nesting can make your code harder to read. If you find yourself nesting loops too deeply, consider breaking your code into functions to simplify it.\n",
            "\n",
            "### 7. **Use meaningful variable names**\n",
            "\n",
            "Make sure the variables used in loops are descriptive so that the code remains readable and maintainable.\n",
            "\n",
            "### Final Thought\n",
            "Choose the loop structure that best suits your needs while maintaining clarity and efficiency. Each type of loop has its appropriate use case, and being clear in your intentions will help anyone reading your code in the future (including yourself!). \n",
            "\n",
            "If you have a specific scenario in mind, feel free to share, and I'd be happy to help you with that!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "You are a seasoned startup coach that helps executives understand and mitigate the challenges facing their startup.\n",
        "You should use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "        return {\"response\" : self.llm.run([formatted_system_prompt, formatted_user_prompt]), \"context\" : context_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through the Week 1 Day 1 \"Prompting OpenAI Like A Developer\" material for an answer to this question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many prompting techniques that could lead to a more detailed response, including chain-of-thought, instructing the system to take on a persona, including examples ('few-shot learning'), trying principle #6 from the \"Principled Instructions\" paper ('6 Add “I’m going to tip $xxx for a better solution!'), instructing the system to \"think step by step\", giving the system some guidelines on answer length and formatting, or others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "kqbE9fZ6R6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGhaCGOR6yz",
        "outputId": "e4fb3a1b-d2bc-4e18-ec31-dc0adf767163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"The 'Michael Eisner Memorial Weak Executive Problem' refers to the tendency of a CEO or startup founder to hire an executive who is weaker than themselves in a specific function, particularly one they were previously involved in. This often occurs because the CEO struggles to let go of the role they were once proficient in, leading them to select someone who is not as competent. The problem is exemplified by Michael Eisner, the former CEO of Disney, who after acquiring ABC, faced difficulties due to a poor performing network executive. The takeaway is to ensure that when hiring for a role that aligns with the founder's former specialty, the new hire should be significantly more capable than the founder in that area.\",\n",
              " 'context': [('ot the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              "   np.float64(0.5165810998881556)),\n",
              "  ('management,\\nmarketing, or sales. You don’t want your key executives focused\\non selling the company — unless of course you want them\\nfocused on selling the company. Make your acceleration decisions accordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — consciously or subconsciously. Don’t let it happen to you — make sure the person you\\nhire into that role is way better than you used to be.\\nEighth, recognize that hiring an executive is a high-risk proposition.\\nYou oaen see a startup with a screwed up development process,\\nbut “when we get our VP of Engineering onboard, everything\\nwill get Xxed”. Or a startup that is missing its revenue targets, but\\n“when we get our VP of sales, reveue will take oW”.\\nHere’s the problem: in my experience, if you know what you’re\\ndoing, the odds of a given executive hire working out will be about\\nPart 8: Hiring, managing, promoting, and Dring executives 59\\n50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\\nyou’re doing, your failure rate will b',\n",
              "   np.float64(0.4867837303349146)),\n",
              "  (' describes a concept\\ncalled “Task Relevant Maturity”. Andy explains that employees\\nwho are immature in a given task require detailed training and\\ninstruction. They need to be micromanaged. On the other\\nhand, if an employee is relatively mature in a task, then it is\\ncounterproductive and annoying to manage the details of their\\nwork.\\nThis is also true when managing executives. Marc might think\\nthat he hires an executive because she has the experience and\\nknow-how to comprehensively do her job, so any detailed\\ninstruction would be unwise and unwarranted. Marc would be\\nwrong about that. It turns out that even — and maybe especially\\n— executives are also immature in certain tasks.\\nIt is almost always the case that a new executive will be immature in their understanding of your market, your technology,\\nand your company — its personnel, processes, and culture. Will\\nthe new head of engineering at Ning walk in the door with\\nMarc’s understanding of the development process or the technology base? Would it be better for this new head of engineering\\nto make guesses and use her own best — not so good– judgment, or for Marc to review the Xrst say 20 decisions until the\\nnew exec is fully up to speed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthe',\n",
              "   np.float64(0.45594117814985397)),\n",
              "  ('Archives\\n• Demotion as an alternative to Xring (or, alternately, “I know,\\nwe’ll hire her a boss!”). Hate it. Great people don’t deal well\\nwith getting demoted. There is an occasional exception.\\nUnless you are positive you have such an exception, skip it,\\nand move directly to the conclusion.\\nFourth, don’t feel guilty.\\nYou’re not beheading anyone.\\nAnyone who got a job as an executive at a startup is going to\\nhave an easy time getting the next job. Aaer all, she can always\\npaint you as a crazy founder, or inept CEO.\\nMore oaen than not, when you Xre an executive, you are doing\\nher a favor — you are giving her a chance to Xnd a better Xt\\nin a diWerent company where she will be more valued, more\\nrespected, and more successful. This sounds mushy, but I mean\\nit. And if she can’t, then she has a much deeper problem and\\nyou just dodged a huge bullet.\\nAnd on that cheery note, good luck!\\nCounterpoint: Ben Horowitz on\\nmicromanagement\\n[This is a guest post from my business partner Ben Horowitz, reacting\\nto my recent post about hiring, managing, promoting, and Fring executives. I have italicized the parts where he really tears into me for your\\nadded humor value.]\\nWhile I enjoyed Marc’s post on hiring and Xring executives, I\\nthink that he unfairly dissed micromanagement.\\nHere’s why.\\nEveryone knows that the hyper-controlling manager with the\\nsevere personality disorder who micromanages every crummy\\ndecision is no fun to work for. However, it is wrong to condemn\\nthe practice of micromanagement on that basis.\\nPart 8: Hiring, managing, promoting, and Dring executives 65\\nSpeciXcally, there are times and situations where micromanaging executives is not just ok, but also the right thing to do.\\nAndy Grove has an excellent explanation of this in his classic\\nbook High Output Management, where he describes a concept\\ncalled “Task Relevant Maturity”. Andy explains that employees\\nwho are immature in a given task require detailed training and\\ninstruction. They need to be micromanaged. On the othe',\n",
              "   np.float64(0.4490606604677736))]}"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the 'Michael Eisner Memorial Weak Executive Problem'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many improvements that could be made to this pipeline! For example:\n",
        "* Spell check the inputs, which appear to have many (intentional?) typos\n",
        "* Context- (or at least word-) aware chunking\n",
        "* A more nuanced and sophisticated system prompt\n",
        "* Metadata support, for example including a date or blog entry title\n",
        "* Adjust the OpenAI parameters to be more optimal for this use case\n",
        "\n",
        "Recently, I have been working with some RAG pipelines that include hybrid search followed by cross encoders. Because cross encoders can be a bit heavyweight / slow depending on the approach, I have been wondering if a simple, more \"classic\" IR approach such as BM25 can work as a reranker. Although this remains a vector-only search, not hybrid, I decided to try this below.  I also made a couple other small tweaks:\n",
        "* Increase the chunk size to 2k characters (about a page of text) to include a little more context\n",
        "* Add a role to the system prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use BM25 to re-rank the retrieved results and see if it helps.\n",
        "!pip install -qU rank_bm25 \n",
        "\n",
        "from rank_bm25 import BM25Plus\n",
        "\n",
        "# Utility function for reranking\n",
        "def bm25plus_rerank(corpus, query, initial_ranking, top_n=3):\n",
        "    tokenized_corpus = [corpus[i].split() for i in initial_ranking]\n",
        "    tokenized_query = query.split()\n",
        "\n",
        "    bm25 = BM25Plus(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "    \n",
        "    ranked_indices = [initial_ranking[i] for i in bm25_scores.argsort()[::-1]]    \n",
        "    return ranked_indices[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    \n",
        "class RerankedQAPipeline(RetrievalAugmentedQAPipeline):\n",
        "    # Extends the RetrievalAugmentedQAPipeline class with reranking\n",
        "    \n",
        "    def run_pipeline(self, user_query: str, rerank: bool=False, system_prompt: str = \"\") -> str:\n",
        "        # Retrieve the top 10 results. Either return the top 3, or rerank with BM25 and then return \n",
        "        # the new top 3\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=10)\n",
        "\n",
        "        # Convert from tuples to strings\n",
        "        context_list_str = [context_list[i][0] for i in range(len(context_list))]\n",
        "\n",
        "        # Optionally re-rank the retrieved context using BM25\n",
        "        n = 3\n",
        "        reranked_contexts = context_list_str[0:n]\n",
        "\n",
        "        if rerank:\n",
        "            initial_ranking = list(range(len(context_list_str)))\n",
        "            reranked_indices = bm25plus_rerank(context_list_str, user_query, initial_ranking, top_n=n)\n",
        "            reranked_contexts = [context_list_str[i] for i in reranked_indices]\n",
        "\n",
        "        context_prompt = \"\\n\\n\".join(context for context in reranked_contexts) + \"\\n\\n\"\n",
        "\n",
        "        formatted_system_prompt = SystemRolePrompt(system_prompt).create_message() if system_prompt else rag_prompt.create_message()\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "        # Step 5: Generate the response using the LLM\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]),\n",
        "            \"context\": reranked_contexts\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Utility function to make it easier to look at the contexts that are selected\n",
        "def print_contexts(context_list: list[str]) -> None:\n",
        "    print(\"**** Context ****\")\n",
        "    for i in range(0,len(context_list)):\n",
        "            print(\"Context\",i,\":\",context_list[i],\"\\n\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Utility function to compare two sets of responses\n",
        "def compare_responses(pipeline:RerankedQAPipeline, query:str, show_context = False) -> None:\n",
        "    no_reranking_response = retrieval_augmented_qa_pipeline.run_pipeline(query, rerank=False)\n",
        "    reranked_response = retrieval_augmented_qa_pipeline.run_pipeline(query, rerank=True)\n",
        "\n",
        "    pprint(f'Response without re-ranking: {no_reranking_response[\"response\"]}')\n",
        "    print(\"\\n\")\n",
        "    pprint(f'Response with re-ranking: {reranked_response[\"response\"]}')\n",
        "\n",
        "    if show_context:\n",
        "        print(\"\\n\\nBefore reranking:\")\n",
        "        print_contexts(no_reranking_response[\"context\"])\n",
        "        print(\"\\n\\nAfter reranking\")\n",
        "        print_contexts(reranked_response[\"context\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"Response without re-ranking: The 'Michael Eisner Memorial Weak Executive \"\n",
            " \"Problem' refers to the tendency of CEOs or startup founders to hire \"\n",
            " 'executives for key functions who are weaker than themselves, particularly in '\n",
            " 'the area that was their own specialty. This often happens because these '\n",
            " 'leaders have difficulty letting go of the functions that initially brought '\n",
            " 'them success. An example provided is Michael Eisner, the former CEO of '\n",
            " 'Disney, who, despite being a brilliant TV network executive, encountered '\n",
            " 'failure after acquiring ABC because he hired someone weak in that role, '\n",
            " 'leading to poor performance. The cautionary tale highlights the importance '\n",
            " 'of hiring individuals who are significantly more capable than the founder or '\n",
            " 'CEO in their respective areas.')\n",
            "\n",
            "\n",
            "('Response with re-ranking: The \"Michael Eisner Memorial Weak Executive '\n",
            " 'Problem\" refers to the tendency of a CEO or startup founder to hire an '\n",
            " 'executive who is weaker than themselves in their area of expertise. This '\n",
            " 'often occurs when a CEO has difficulty letting go of the function that '\n",
            " 'contributed to their success and, either consciously or subconsciously, '\n",
            " 'hires someone less capable to ensure they remain the top authority in that '\n",
            " 'area. This can lead to poor performance and decision-making, as the newly '\n",
            " 'hired executive may not be strong enough to lead effectively. The term is '\n",
            " 'named after Michael Eisner, the former CEO of Disney, who faced challenges '\n",
            " 'with ABC after acquiring it, highlighting the risks of such hiring '\n",
            " 'decisions.')\n",
            "\n",
            "\n",
            "Before reranking:\n",
            "**** Context ****\n",
            "Context 0 : ot the only time that it makes sense to micro66 The Pmarca Blog Archives\n",
            "manage executives. It turns out that just about every executive\n",
            "in the world has a few things that are seriously wrong with\n",
            "them. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\n",
            "at everything. When hiring and when Hring executives, you\n",
            "must therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\n",
            "them yet. When managing, it’s oaen useful to micromanage and\n",
            "to provide remedial training around these weaknesses. Doing so\n",
            "may make the diWerence between an executive succeeding or\n",
            "failing.\n",
            "For example, you might have a brilliant engineering executive\n",
            "who generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\n",
            "may be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\n",
            "signiXcantly impede your ability to sell and market eWectively.\n",
            "Your alternatives are:\n",
            "(a) Macro-manage and give her an annual or quarterly objective\n",
            "to Xx it, or…\n",
            "(b) Intensively micromanage her interactions until she learns\n",
            "the fundamental interpersonal skills required to be an eWective\n",
            "executive.\n",
            "I am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\n",
            "eWective with her peers. If somebody is an executive, it’s very\n",
            "likely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\n",
            "the weakness remains. As a result, executives generally require\n",
            "more hands-on management than lower level employees to\n",
            "improve weak areas.\n",
            "So, micromanagement is like Xne wine. A little at the right times\n",
            "will really enhance things; too much all the time and you’ll end\n",
            "up in rehab.\n",
            "Part 8: Hiring, managing, promoting, and Dring execut \n",
            "\n",
            "Context 1 : management,\n",
            "marketing, or sales. You don’t want your key executives focused\n",
            "on selling the company — unless of course you want them\n",
            "focused on selling the company. Make your acceleration decisions accordingly.\n",
            "Seventh, when hiring the executive to run your former specialty, be\n",
            "careful you don’t hire someone weak on purpose.\n",
            "This sounds silly, but you wouldn’t believe how oaen it happens.\n",
            "The CEO who used to be a product manager who has a weak\n",
            "product management executive. The CEO who used to be in\n",
            "sales who has a weak sales executive. The CEO who used to be\n",
            "in marketing who has a weak marketing executive.\n",
            "I call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\n",
            "promptly fell to fourth place. His response? “If I had an extra\n",
            "two days a week, I could turn around ABC myself.” Well, guess\n",
            "what, he didn’t have an extra two days a week.\n",
            "A CEO — or a startup founder — oaen has a hard time letting\n",
            "go of the function that brought him to the party. The result: you\n",
            "hire someone weak into the executive role for that function so\n",
            "that you can continue to be “the man” — consciously or subconsciously. Don’t let it happen to you — make sure the person you\n",
            "hire into that role is way better than you used to be.\n",
            "Eighth, recognize that hiring an executive is a high-risk proposition.\n",
            "You oaen see a startup with a screwed up development process,\n",
            "but “when we get our VP of Engineering onboard, everything\n",
            "will get Xxed”. Or a startup that is missing its revenue targets, but\n",
            "“when we get our VP of sales, reveue will take oW”.\n",
            "Here’s the problem: in my experience, if you know what you’re\n",
            "doing, the odds of a given executive hire working out will be about\n",
            "Part 8: Hiring, managing, promoting, and Dring executives 59\n",
            "50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\n",
            "you’re doing, your failure rate will b \n",
            "\n",
            "Context 2 :  describes a concept\n",
            "called “Task Relevant Maturity”. Andy explains that employees\n",
            "who are immature in a given task require detailed training and\n",
            "instruction. They need to be micromanaged. On the other\n",
            "hand, if an employee is relatively mature in a task, then it is\n",
            "counterproductive and annoying to manage the details of their\n",
            "work.\n",
            "This is also true when managing executives. Marc might think\n",
            "that he hires an executive because she has the experience and\n",
            "know-how to comprehensively do her job, so any detailed\n",
            "instruction would be unwise and unwarranted. Marc would be\n",
            "wrong about that. It turns out that even — and maybe especially\n",
            "— executives are also immature in certain tasks.\n",
            "It is almost always the case that a new executive will be immature in their understanding of your market, your technology,\n",
            "and your company — its personnel, processes, and culture. Will\n",
            "the new head of engineering at Ning walk in the door with\n",
            "Marc’s understanding of the development process or the technology base? Would it be better for this new head of engineering\n",
            "to make guesses and use her own best — not so good– judgment, or for Marc to review the Xrst say 20 decisions until the\n",
            "new exec is fully up to speed?\n",
            "In reality — as opposed to Marc’s warped view of reality — it will\n",
            "be extremely helpful for Marc [if he were actually the CEO,\n",
            "which he is not] to meet with the new head of engineering daily\n",
            "when she comes on board and review all of her thinking and\n",
            "decisions. This level of micromanagement will accelerate her\n",
            "training and improve her long-term eWectiveness. It will make\n",
            "her seem smarter to the rest of the organization which will build\n",
            "credibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\n",
            "period of time.\n",
            "However, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\n",
            "manage executives. It turns out that just about every executive\n",
            "in the world has a few things that are seriously wrong with\n",
            "the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "After reranking\n",
            "**** Context ****\n",
            "Context 0 : management,\n",
            "marketing, or sales. You don’t want your key executives focused\n",
            "on selling the company — unless of course you want them\n",
            "focused on selling the company. Make your acceleration decisions accordingly.\n",
            "Seventh, when hiring the executive to run your former specialty, be\n",
            "careful you don’t hire someone weak on purpose.\n",
            "This sounds silly, but you wouldn’t believe how oaen it happens.\n",
            "The CEO who used to be a product manager who has a weak\n",
            "product management executive. The CEO who used to be in\n",
            "sales who has a weak sales executive. The CEO who used to be\n",
            "in marketing who has a weak marketing executive.\n",
            "I call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\n",
            "promptly fell to fourth place. His response? “If I had an extra\n",
            "two days a week, I could turn around ABC myself.” Well, guess\n",
            "what, he didn’t have an extra two days a week.\n",
            "A CEO — or a startup founder — oaen has a hard time letting\n",
            "go of the function that brought him to the party. The result: you\n",
            "hire someone weak into the executive role for that function so\n",
            "that you can continue to be “the man” — consciously or subconsciously. Don’t let it happen to you — make sure the person you\n",
            "hire into that role is way better than you used to be.\n",
            "Eighth, recognize that hiring an executive is a high-risk proposition.\n",
            "You oaen see a startup with a screwed up development process,\n",
            "but “when we get our VP of Engineering onboard, everything\n",
            "will get Xxed”. Or a startup that is missing its revenue targets, but\n",
            "“when we get our VP of sales, reveue will take oW”.\n",
            "Here’s the problem: in my experience, if you know what you’re\n",
            "doing, the odds of a given executive hire working out will be about\n",
            "Part 8: Hiring, managing, promoting, and Dring executives 59\n",
            "50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\n",
            "you’re doing, your failure rate will b \n",
            "\n",
            "Context 1 : tartup, an executive is hired to take over a function that’s\n",
            "already been started, at least in rudimentary form. The\n",
            "people in that function should be noticeably better at their\n",
            "jobs, and highly respectful of the executive’s skills, within the\n",
            "Xrst several months at the very least. If not, you have a\n",
            "problem.\n",
            "• What do the other executives think?Great executives are oaen\n",
            "imperfect but their peers always respect them. If your other\n",
            "executives are skeptical of a new executive aaer the Xrst few\n",
            "months, you have a problem.\n",
            "• Is it painful for you to interact with the executive?Do you try to\n",
            "avoid or cancel your 1:1’s? Does talking to her give you a\n",
            "headache? Do you oaen not understand what point she’s\n",
            "trying to make or why she’s focused on such an odd issue? If\n",
            "the answer to any of these questions is yes, you have a\n",
            "problem.\n",
            "Third, Fre crisply.\n",
            "Firing an executive sucks. It’s disruptive to the organization. It\n",
            "creates a lot of work for you — not least of which is you’ll have\n",
            "to go Xnd someone else for the job. And, it risks making you\n",
            "look bad, since you’re the one who hired the person in the Xrst\n",
            "place.\n",
            "And it always seems to happen at a critical time in your startup’s\n",
            "life, when the last thing you need is a distraction like this.\n",
            "Nevertheless, the only thing to do is do it, do it professionally,\n",
            "make clear to the organization what will happen next, and get\n",
            "on down the road.\n",
            "In my opinion the two most common mistakes people make\n",
            "when they Xre executives both fall in the category of pulling\n",
            "one’s punches, and I highly recommend avoiding them:\n",
            "• Long transition periods — tempting, but counterproductive.\n",
            "Confusing, demoralizing, and just plain weird. Instead, make\n",
            "a clean break, put a new person in charge — even if only on\n",
            "an acting basis — and get moving.\n",
            "64 The Pmarca Blog Archives\n",
            "• Demotion as an alternative to Xring (or, alternately, “I know,\n",
            "we’ll hire her a boss!”). Hate it. Great people don’t deal well\n",
            "with getting demoted. There is an occasional exception.\n",
            "Unless \n",
            "\n",
            "Context 2 :  describes a concept\n",
            "called “Task Relevant Maturity”. Andy explains that employees\n",
            "who are immature in a given task require detailed training and\n",
            "instruction. They need to be micromanaged. On the other\n",
            "hand, if an employee is relatively mature in a task, then it is\n",
            "counterproductive and annoying to manage the details of their\n",
            "work.\n",
            "This is also true when managing executives. Marc might think\n",
            "that he hires an executive because she has the experience and\n",
            "know-how to comprehensively do her job, so any detailed\n",
            "instruction would be unwise and unwarranted. Marc would be\n",
            "wrong about that. It turns out that even — and maybe especially\n",
            "— executives are also immature in certain tasks.\n",
            "It is almost always the case that a new executive will be immature in their understanding of your market, your technology,\n",
            "and your company — its personnel, processes, and culture. Will\n",
            "the new head of engineering at Ning walk in the door with\n",
            "Marc’s understanding of the development process or the technology base? Would it be better for this new head of engineering\n",
            "to make guesses and use her own best — not so good– judgment, or for Marc to review the Xrst say 20 decisions until the\n",
            "new exec is fully up to speed?\n",
            "In reality — as opposed to Marc’s warped view of reality — it will\n",
            "be extremely helpful for Marc [if he were actually the CEO,\n",
            "which he is not] to meet with the new head of engineering daily\n",
            "when she comes on board and review all of her thinking and\n",
            "decisions. This level of micromanagement will accelerate her\n",
            "training and improve her long-term eWectiveness. It will make\n",
            "her seem smarter to the rest of the organization which will build\n",
            "credibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\n",
            "period of time.\n",
            "However, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\n",
            "manage executives. It turns out that just about every executive\n",
            "in the world has a few things that are seriously wrong with\n",
            "the \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline = RerankedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai\n",
        ")\n",
        "\n",
        "# Compare responses before and after re-ranking. The responses are pretty similar. \n",
        "query = \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\"\n",
        "compare_responses(retrieval_augmented_qa_pipeline,query, show_context=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Response without re-ranking: The three risks that startups face include:\\n'\n",
            " '\\n'\n",
            " '1. Founder risk — does the startup have the right founding team? This '\n",
            " 'involves assessing whether the technologist is capable and whether the '\n",
            " 'business person can effectively run the company.\\n'\n",
            " '\\n'\n",
            " '2. Market risk — is there a market for the product? This includes '\n",
            " 'determining if anyone will want the product, if they will pay for it, and '\n",
            " 'how much they will pay.\\n'\n",
            " '\\n'\n",
            " '3. Competition risk — are there too many other startups already doing this? '\n",
            " 'This involves evaluating whether the startup is sufficiently differentiated '\n",
            " 'from competitors and large incumbents.')\n",
            "\n",
            "\n",
            "('Response with re-ranking: The three risks that startups face, as outlined in '\n",
            " 'the provided context, include:\\n'\n",
            " '\\n'\n",
            " \"1. Founder risk - This involves concerns about whether the founding team's \"\n",
            " 'skills and capabilities are sufficient for leading the startup, particularly '\n",
            " 'the roles of the technologist and the business person.\\n'\n",
            " '\\n'\n",
            " '2. Market risk - This pertains to the need for startups to validate their '\n",
            " 'market, ideally by acquiring paying customers or credible prospects to '\n",
            " 'demonstrate that the market exists.\\n'\n",
            " '\\n'\n",
            " \"3. Competition risk - This concerns whether the startup's differentiation \"\n",
            " 'from competitors is strong enough to ensure success against other existing '\n",
            " 'players in the market.')\n"
          ]
        }
      ],
      "source": [
        "# Try another query. Again, very similar responses\n",
        "query = \"What three risks that startups face?\"\n",
        "compare_responses(retrieval_augmented_qa_pipeline,query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Response without re-ranking: The role of luck in entrepreneurship is '\n",
            " 'significant, as many successful entrepreneurs acknowledge that luck plays a '\n",
            " 'huge part in the difference between success and failure. However, this '\n",
            " 'acknowledgment is often made reluctantly because admitting the role of luck '\n",
            " 'can undermine the image of the entrepreneur as a business genius.\\n'\n",
            " '\\n'\n",
            " 'According to Dr. James Austin\\'s theory outlined in his book \"Chase, Chance, '\n",
            " 'and Creativity,\" there are four kinds of luck:\\n'\n",
            " '\\n'\n",
            " '1. **Chance I**: Completely impersonal luck that cannot be influenced.\\n'\n",
            " '2. **Chance II**: Luck that favors those who have a persistent curiosity and '\n",
            " 'are willing to experiment.\\n'\n",
            " '3. **Chance III**: Luck that benefits those with a strong background of '\n",
            " 'knowledge and the ability to form new associations quickly.\\n'\n",
            " '4. **Chance IV**: Luck that favors individuals with unique hobbies and '\n",
            " 'lifestyles, allowing them to approach problems from unconventional angles.\\n'\n",
            " '\\n'\n",
            " 'Thus, while some aspects of luck are beyond control, there are ways '\n",
            " 'entrepreneurs can increase their likelihood of encountering favorable '\n",
            " 'circumstances through curiosity, energetic experimentation, broad '\n",
            " 'experiences, and a distinctive personal perspective.')\n",
            "\n",
            "\n",
            "('Response with re-ranking: The role of luck in entrepreneurship is '\n",
            " 'significant, as many successful entrepreneurs acknowledge that luck plays a '\n",
            " 'huge part in the difference between success and failure. While some may '\n",
            " 'hesitate to admit this due to the desire to maintain an image of being an '\n",
            " 'all-knowing business genius, luck is recognized as an unpredictable force '\n",
            " \"that is often beyond a person's control. \\n\"\n",
            " '\\n'\n",
            " 'Dr. James Austin describes luck as something fortuitous that happens '\n",
            " 'unpredictably without discernible human intention. However, luck can still '\n",
            " 'be influenced by human interventions, although those interventions should be '\n",
            " 'viewed as accidental and inadvertent rather than intentional.\\n'\n",
            " '\\n'\n",
            " 'Essentially, while luck is a crucial element, it is not the only factor in '\n",
            " 'entrepreneurial success, but it can often overshadow the other operational '\n",
            " 'elements such as product/market fit. Successful startups often find that '\n",
            " 'reaching product/market fit is the key determinant of their success, even if '\n",
            " 'they made mistakes in other areas along the way.')\n"
          ]
        }
      ],
      "source": [
        "query = \"What's the role of luck in entrepreneurship\"\n",
        "compare_responses(retrieval_augmented_qa_pipeline,query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, it really isn't clear that re-ranking with BM25 leads to much improvement in responses, although I personally prefer the re-ranked response in the last example. Without a rigorous test dataset in place, it is hard to tell. Based on these results, I would probably consider other re-ranking methods first in the future."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "buildyourownlangchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
