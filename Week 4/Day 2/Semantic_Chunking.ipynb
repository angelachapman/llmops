{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U -q langchain langchain-openai langchain_core langchain-community langchainhub openai langchain-qdrant\n",
    "!pip install -qU ragas\n",
    "!pip install -qU qdrant-client pymupdf pandas\n",
    "!pip install -qU nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And set the OpenAI key\n",
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up a document to work with -- in this case, a series of blog posts about entrepreneurship\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from pprint import pprint\n",
    "\n",
    "PDF_LINK = \"https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path=PDF_LINK,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# visual inspection shows that page 7 (zero-indexed page 6) is pretty much the first page with any meaningful text, let's start there.\n",
    "documents=documents[6:]\n",
    "\n",
    "# Each document will now contain text extracted as blocks (pages)\n",
    "for doc in documents[:10]:\n",
    "    pprint(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to have artificial page-breaks, so let's recombine those into one long document before we start semantic chunking. It also looks like pymupdfloader didn't give us a good way to distinguish between paragraphs, as every line is just separated by a newline. To avoid spending a lot of time on pdf munging, let's just strip out the newlines and treat it as one long text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "one_big_string = \"\"\n",
    "for doc in documents:\n",
    "    cleaned_content = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "    one_big_string += cleaned_content\n",
    "\n",
    "print(one_big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Part 1: Why not to do a startup In this series of posts I will walk through some of my accumu- lated knowledge and experience in building high-tech startups.\n",
      "\n",
      "\n",
      "sentence: My speciXc experience is from three companies I have co- founded: Netscape, sold to America Online in 1998 for $4.2 billion; Opsware (formerly Loudcloud), a public soaware com- pany with an approximately $1 billion market cap; and now Ning, a new, private consumer Internet company.\n",
      "\n",
      "\n",
      "sentence: But more generally, I’ve been fortunate enough to be involved in and exposed to a broad range of other startups — maybe 40 or 50 in enough detail to know what I’m talking about — since arriving in Silicon Valley in 1994: as a board member, as an angel investor, as an advisor, as a friend of various founders, and as a participant in various venture capital funds.\n",
      "\n",
      "\n",
      "sentence: This series will focus on lessons learned from this entire cross- section of Silicon Valley startups — so don’t think that anything I am talking about is referring to one of my own companies: most likely when I talk about a scenario I have seen or some- thing I have experienced, it is from some other startup that I am not naming but was involved with some other way than as a founder.\n",
      "\n",
      "\n",
      "sentence: Finally, much of my perspective is based on Silicon Valley and the environment that we have here — the culture, the people, the venture capital base, and so on.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Angela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/Angela/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Split into sentences\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download sentence tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Create the sentences as a list\n",
    "sentences=sent_tokenize(one_big_string)\n",
    "\n",
    "# Print out a few as a sanity check\n",
    "for sentence in sentences[:5]:\n",
    "    print(f\"sentence: {sentence}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find semantically similar sentences. I'm going to do it a naive way, using OpenAI's text-embedding-ada-3 and setting a cosine similarity threshold of >0.4 for \"related\" sentences, with the threshold based on quick trial and error. This isn't the most robust method but should be fine for a proof of concept. There are plenty of other methods we could try, and Langchain has several in its implementation.\n",
    "\n",
    "First, we need to get the embeddings for each sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# set up embedding model and some constants \n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "   model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "sentence_embeddings = await embeddings.aembed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILARITY_THRESHOLD = 0.4 # cosine similarity threshold\n",
    "MAX_CHUNK_SIZE=1000 # max chunk size, although we can go over it to preserve a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for cosine similarity\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's chunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main cell that does semantic chunking. Check the relatedness of each sentence pair and greedily grow chunks to maximum size.\n",
    "combined_chunks = []\n",
    "this_chunk = sentences[0]\n",
    "\n",
    "for i in range(len(sentences)-1):\n",
    "\n",
    "    similarity = cosine_similarity(sentence_embeddings[i],sentence_embeddings[i+1])\n",
    "\n",
    "    if (len(this_chunk) > MAX_CHUNK_SIZE) or (similarity<SIMILARITY_THRESHOLD):\n",
    "        # we are over the max chunk size or sentences are unrelated, time to start a new one\n",
    "        if this_chunk != \"\": combined_chunks.append(this_chunk)\n",
    "        this_chunk = sentences[i+1]\n",
    "    else:\n",
    "        this_chunk += (\"  \")+sentences[i+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 1621\n",
      "\n",
      "Smallest chunk (3 chars):No.\n",
      "\n",
      "Biggest chunk (1879 chars):That’s an extreme case, but even a non-extreme version of this process — and all big companies have one; they have to — is mind-bend- ingly complex to try to understand, even from the inside, let alone the outside.  “… and the breath of the whale is frequently attended with such an insupportable smell, as to bring on a disorder of the brain.” — Ulloa’s South America You can count on there being a whole host of impinging forces that will aWect the dynamic of decision-making on any issue at a big company.  The consensus building process, trade-oWs, quids pro quo, poli- tics, rivalries, arguments, mentorships, revenge for past wrongs, Part 5: The Moby Dick theory of big companies 35turf-building, engineering groups, product managers, product marketers, sales, corporate marketing, Xnance, HR, legal, chan- nels, business development, the strategy team, the international divisions, investors, Wall Street analysts, industry analysts, good press, bad press, press articles being written that you don’t know about, customers, prospects, lost sales, prospects on the fence, partners, this quarter’s sales numbers, this quarter’s margins, the bond rating, the planning meeting that happened last week, the planning meeting that got cancelled this week, bonus programs, people joining the company, people leaving the company, peo- ple getting Xred by the company, people getting promoted, peo- ple getting sidelined, people getting demoted, who’s sleeping with whom, which dinner party the CEO went to last night, the guy who prepares the Powerpoint presentation for the staW meeting accidentally putting your startup’s name in too small a font to be read from the back of the conference room… You can’t possibly even identify all the factors that will come to bear on a big company’s decision, much less try to understand them, much less try to inYuence them very much at all.\n",
      "\n",
      "Chunk: Part 1: Why not to do a startup In this series of posts I will walk through some of my accumu- lated knowledge and experience in building high-tech startups.\n",
      "\n",
      "Chunk: My speciXc experience is from three companies I have co- founded: Netscape, sold to America Online in 1998 for $4.2 billion; Opsware (formerly Loudcloud), a public soaware com- pany with an approximately $1 billion market cap; and now Ning, a new, private consumer Internet company.\n",
      "\n",
      "Chunk: But more generally, I’ve been fortunate enough to be involved in and exposed to a broad range of other startups — maybe 40 or 50 in enough detail to know what I’m talking about — since arriving in Silicon Valley in 1994: as a board member, as an angel investor, as an advisor, as a friend of various founders, and as a participant in various venture capital funds.  This series will focus on lessons learned from this entire cross- section of Silicon Valley startups — so don’t think that anything I am talking about is referring to one of my own companies: most likely when I talk about a scenario I have seen or some- thing I have experienced, it is from some other startup that I am not naming but was involved with some other way than as a founder.  Finally, much of my perspective is based on Silicon Valley and the environment that we have here — the culture, the people, the venture capital base, and so on.\n",
      "\n",
      "Chunk: Some of it will travel wellto other regions and countries, some probably will not.\n",
      "\n",
      "Chunk: Caveat emptor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a sense for how the chunks look now\n",
    "print(f\"Num chunks: {len(combined_chunks)}\\n\")\n",
    "\n",
    "smallest_chunk = min(combined_chunks,key=len)\n",
    "largest_chunk = max(combined_chunks, key=len)\n",
    "\n",
    "print(f\"Smallest chunk ({len(smallest_chunk)} chars):{smallest_chunk}\\n\")\n",
    "print(f\"Biggest chunk ({len(largest_chunk)} chars):{largest_chunk}\\n\")\n",
    "for chunk in combined_chunks[:5]:\n",
    "    print(f\"Chunk: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having orphan chunks like \"No\" isn't great. In practice you probably want some minimum chunk size, but the longer chunks seem to be coherent and topical. Let's try it out as is.\n",
    "\n",
    "Create a RAG pipeline (I've written my own wrapper but you could also use Langchain's stuff_documents_chain). Grab the test data we previously used in class and also copy over the code to run RAGAS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "populated vector db\n",
      "created chain\n",
      "{'response': AIMessage(content=\"A good rule of thumb to follow when selecting an industry to invest in is to choose an industry where the founders of the important companies are still alive and actively involved. Additionally, if you're entering an old industry, make sure to align with the forces of radical change that could disrupt the existing order. Once you've picked an industry, aim to get to the center of it quickly, focusing on the core of change and opportunity.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 553, 'total_tokens': 637}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd5eb1cf-3cf7-4e02-ad01-a029f0682ba0-0', usage_metadata={'input_tokens': 553, 'output_tokens': 84, 'total_tokens': 637}), 'context': [Document(metadata={'_id': '9fd58a66555f47248977e6ffe4b2e208', '_collection_name': 'PMarca Blogs'}, page_content='If you’re going to enter an old industry, make sure to do it on the side of the forces of radical change that threaten to up-end the existing order — and make sure that those forces of change have a reasonable chance at succeeding.  Second rule of thumb: Once you have picked an industry, get right to the center of it as fast as you possibly can.  Your target is the core of change and opportunity — Xgure out where the action is and head there, and do not delay your progress for extraneous opportunities, no matter how lucrative they might be.'), Document(metadata={'_id': '8152dc1b3ad641d5bab96aa03a72ebd9', '_collection_name': 'PMarca Blogs'}, page_content='Part 2: Skills and education 119Part 3: Where to go and why When picking an industry to enter, my favorite rule of thumb is this: Pick an industry where the founders of the industry — the founders of the important companies in the industry — are still alive and actively involved.'), Document(metadata={'_id': '7b4a0af1f78045f28d311b336829b453', '_collection_name': 'PMarca Blogs'}, page_content='Apply this rule when selecting which company to go to.  Go to the company where all the action is happening.  Or, if you are going to join a startup or start your own company, always make sure that your startup is aimed at the largest and Part 3: Where to go and why 121most interesting opportunity available — the new markets that are growing fast and changing rapidly.'), Document(metadata={'_id': 'ccde616a3b964dada6c8f291d6f9de8f', '_collection_name': 'PMarca Blogs'}, page_content='Don’t fart around in second and third tier companies that don’t have a clear mission to dominate their markets.  Third rule: In a rapidly changing Held like technology, the best place to get experience when you’re starting out is in younger, high- growth companies.  (This is not necessarily true in older and more established industries, but those aren’t the industries we’re talking about.)')]}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import vanilla_rag\n",
    "\n",
    "importlib.reload(vanilla_rag)\n",
    "\n",
    "rag_pipeline = await vanilla_rag.vanilla_rag(combined_chunks, openai.api_key)\n",
    "\n",
    "# Test it out\n",
    "response = await rag_pipeline.ainvoke({\"input\":\"What is a good rule of thumb to follow when selecting an industry to invest in?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the test set and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test set\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answers to evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = await rag_pipeline.ainvoke({\"input\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc7d77ca3064bd09727426d3e5430e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8054, 'answer_relevancy': 0.8152, 'context_recall': 0.8604, 'context_precision': 0.6711, 'answer_correctness': 0.5325}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate target metrics\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]\n",
    "\n",
    "results = evaluate(response_dataset, metrics)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results from some other runs with the same data set.\n",
    "\n",
    "- Baseline with ada 2: \n",
    "{'faithfulness': 0.7181, 'answer_relevancy': 0.8632, 'context_recall': 0.7539, 'context_precision': 0.6594, 'answer_correctness': 0.5941}\n",
    "\n",
    "- Same thing with TE3: \n",
    "{'faithfulness': 0.5940, 'answer_relevancy': 0.8591, 'context_recall': 0.8167, 'context_precision': 0.6930, 'answer_correctness': 0.5590}\n",
    "\n",
    "- With compression retriever, using TE3 embeddings and gpt-4o-mini as compressor: \n",
    "{'faithfulness': 0.5662, 'answer_relevancy': 0.8607, 'context_recall': 0.5554, 'context_precision': 0.6535, 'answer_correctness': 0.5863}\n",
    "\n",
    "- With parent doc retriever using TE3:\n",
    "{'faithfulness': 0.8329, 'answer_relevancy': 0.8994, 'context_recall': 1.0000, 'context_precision': 0.7632, 'answer_correctness': 0.5469}\n",
    "\n",
    "- With simple semantic chunker using TE3:\n",
    "{'faithfulness': 0.8054, 'answer_relevancy': 0.8152, 'context_recall': 0.8604, 'context_precision': 0.6711, 'answer_correctness': 0.5325}\n",
    "\n",
    "It looks like the parent doc retriever still wins, with the best context-related metrics. The simple semantic chunker comes in second for context_recall and third for context_precision. Presumably, other other metrics such as relevancy could be improved with a better prompt or primary model. The semantic chunker itself can certainly be improved from the version in this notebook.\n",
    "\n",
    "These metrics give us some info, but it's also worth considering that at scale, the parent doc retriever could be more expensive or slower due to the extra tokens, so it would still be important to include use case considerations when making a final decision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
